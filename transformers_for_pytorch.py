# -*- coding: utf-8 -*-
"""Transformers-for-Pytorch.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1omWrz_r4SYqDPj0TglWgEmYR3HHA1kkr
"""

!pip install -q datasets

from datasets import load_dataset

dataset = load_dataset("rotten_tomatoes")

from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased")

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")

def tokenize_dataset(dataset):
    return tokenizer(dataset["text"])

dataset = dataset.map(tokenize_dataset, batched=True)

from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

from transformers import TrainingArguments

training_args=TrainingArguments( #Buradaki arg√ºmanlar hiper parametrelerdir.
    output_dir = "my_bert_model",
    learning_rate = 2e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=2,
    report_to = "none"
)

from transformers import Trainer

trainer = Trainer(
    model = model,
    args = training_args,
    train_dataset = dataset["train"],
    eval_dataset = dataset["test"],
    tokenizer = tokenizer,
    data_collator = data_collator
)

trainer.train()

text = "I love NLP. It's fun to analyze the NLP tasks with HuggingFace"

inputs = tokenizer(text, return_tensors = "pt")
inputs

model_path="/kaggle/working/my_bert_model/checkpoint-1000"

model = AutoModelForSequenceClassification.from_pretrained(
    model_path, num_labels=2
)

import torch

with torch.no_grad():
    logits = model(**inputs).logits

logits.argmax().item()

