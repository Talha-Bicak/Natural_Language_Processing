# -*- coding: utf-8 -*-
"""Emotion-Analysis-with-HuggingFace.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1G8cUAVGoaEDMNu0aW1wHewHerP7DVwB9
"""

!pip install -q datasets

from datasets import load_dataset

emotions = load_dataset("dair-ai/emotion")

emotions

train_ds = emotions["train"]
train_ds

len(train_ds)

train_ds[1]

train_ds.column_names

train_ds.features

train_ds[:5]

train_ds["text"][:5]

import pandas as pd

emotions.set_format(type="pandas")

df=emotions["train"][:]
df.head()

def label_int2str(row):
    return emotions["train"].features["label"].int2str(row)

df["label_name"] = df["label"].apply(label_int2str)

df.head()

import matplotlib.pyplot as plt

df["label_name"].value_counts(ascending=True).plot.barh()
plt.title("Frequency of Classes")
plt.show()

df["Words Per Tweet"] = df["text"].str.split().apply(len)
df.boxplot("Words Per Tweet", by="label_name", grid = False, showfliers = False,
          color = "black")
plt.suptitle("")
plt.xlabel("")
plt.show()

emotions.reset_format()

text = "It is fun to work with NLP using HuggingFace."

tokenized_text = list(text)

print(tokenized_text)

token2idx = {ch: idx for idx, ch in enumerate(sorted(set(tokenized_text)))}
print(token2idx)

input_ids=[token2idx[token] for token in tokenized_text]
print(input_ids)

df = pd.DataFrame({"name":["can", "efe","ada"],
                  "label":[0,1,2]})
df

pd.get_dummies(df, dtype=int)

import torch

input_ids = torch.tensor(input_ids)

import torch.nn.functional as F

one_hot_encodings = F.one_hot(input_ids, num_classes=len(token2idx))

one_hot_encodings.shape

print(f"Token:{tokenized_text[0]}")

print(f"Tensor index: {input_ids[0]}")

print(f"One-hot: {one_hot_encodings[0]}")

tokenized_text = text.split()
print(tokenized_text)

from transformers import AutoTokenizer

model_ckpt = "distilbert-base-uncased"

tokenizer = AutoTokenizer.from_pretrained(model_ckpt)

from transformers import DistilBertTokenizer

distbert_tokenize=DistilBertTokenizer.from_pretrained(model_ckpt)

encoded_text = tokenizer(text)
print(encoded_text)

tokens = tokenizer.convert_ids_to_tokens(encoded_text.input_ids)
print(tokens)

tokenizer.convert_tokens_to_string(tokens)

tokenizer.vocab_size

tokenizer.model_max_length

def tokenize(batch):
    return tokenizer(batch["text"], truncation=True)

print(tokenize(emotions["train"][:2]))

emotions_encoded = emotions.map(tokenize, batched=True,
                               batch_size=None)

from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer = tokenizer)

emotions_encoded["train"].column_names

