# -*- coding: utf-8 -*-
"""transformers-for-tensorflow.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1g_GkpojwfyIOE93a4fpaTQMw7gqTW-G2
"""

!pip install -q datasets

from datasets import load_dataset

dataset = load_dataset("rotten_tomatoes")

dataset

dataset["test"][0]

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(
    "distilbert-base-uncased"
)

tokenizer(dataset["train"][0]["text"])

def preprocess_function(examples):
    return tokenizer(examples["text"], truncation = True) # truncation parametresi metni belirli bir uzunluğa kadar alır gerisini keser. Amac ise giriş metnini bel,rli bir uzunlukta tutmaktır.

dataset = dataset.map(preprocess_function, batched=True)

from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer,
                                       return_tensors = "tf")

from transformers import TFAutoModelForSequenceClassification

my_model = TFAutoModelForSequenceClassification.from_pretrained(
    "distilbert-base-uncased"
)

tf_train_set = my_model.prepare_tf_dataset(
    dataset["train"],
    shuffle = True, # Veriyi karıştırmak için kullanılır.
    batch_size = 16,
    collate_fn = data_collator #Dinamik bir şekilde metni paddinglemek için kullanılır.
)

tf_validation_set = my_model.prepare_tf_dataset(
    dataset["validation"],
    shuffle = False,
    batch_size = 16,
    collate_fn = data_collator
)

from tensorflow.keras.optimizers import Adam

my_model.compile(optimizer=Adam(3e-5))

my_model.fit(
    x=tf_train_set,
    validation_data = tf_validation_set,
    epochs=2
)

text = "I love NLP. It's fun to analyze NLP tasks with Hugging Face"

tokenized_text = tokenizer(text, return_tensors="tf")
tokenized_text

logits = my_model(**tokenized_text).logits

from tensorflow import math

int(math.argmax(logits, axis=-1)[0])

